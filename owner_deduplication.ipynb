{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dedupe\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residential Land Use Codes from MA Dept of Revenue\n",
    "# https://www.mass.gov/files/documents/2016/08/wr/classificationcodebook.pdf\n",
    "# Codes are 101*-109*, 031*, and 013*\n",
    "# Often include suffixes (letters, zeroes or no character), thus regex *?\n",
    "USE_CODES = '^1[0-1][1-9]*?|^013*?|^031*?'\n",
    "def read_res(file_list, uses = USE_CODES):\n",
    "    df = pd.DataFrame()\n",
    "    for file in file_list:\n",
    "        df = df.append(gpd.read_file(file), ignore_index=True)\n",
    "    df = df[df['USE_CODE'].str.contains(uses, regex=True)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from MassGIS Standardized Assessor's Parcels\n",
    "# https://docs.digital.mass.gov/dataset/massgis-data-standardized-assessors-parcels\n",
    "# Medford, Cambridge, and Somerville all last updated FY 2019\n",
    "files = ['data/som_assess.dbf', 'data/cam_assess.dbf', 'data/med_assess.dbf']\n",
    "df = read_res(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean owner names!\n",
    "# TODO: This doesn't seem to work in order, leading to 'EE', 'EES', etc.\n",
    "# Replace in order!\n",
    "replace_list = ['FAMILY', 'IRREVOCABLE', 'NOMINEE', 'REVOCABLE', \n",
    "                'REALTY', 'REAL ESTATE', 'TRUSTEES OF', 'TRUSTEE OF', 'TR.' \n",
    "                'TRUSTEE', 'TRST', 'TRUST', 'LTD', 'LLC', 'HOLDINGS', 'REALTORS', 'LIMITED PARTNERSHIP', \n",
    "                'FOR LIFE', 'LIFE ESTATE OF', 'ESTATE OF', 'ESTATE']\n",
    "def clean(c):\n",
    "    c = c.replace('|'.join(map(re.escape, replace_list)), '', regex=True)\n",
    "    return c\n",
    "\n",
    "df['OWN_NAME_CL'] = clean(df['OWNER1'])\n",
    "# Create full concatenated address column.\n",
    "df['ADDR_FULL'] = [', '.join(filter(None, (str(a), str(b), str(c), d))) for a, b, c, d in zip(df['OWN_ADDR'], df['OWN_CITY'], df['OWN_STATE'], df['OWN_CO'])]\n",
    "# Set 'LOC_ID' as the index - unique identifier.\n",
    "df.set_index('LOC_ID')\n",
    "# Drop (empty) geometry column.\n",
    "df.drop('geometry', inplace=True, axis=1)\n",
    "# Convert to dictionary (expected by Dedupe)\n",
    "df_dict = df.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (sameSevenCharStartPredicate, OWN_NAME_CL), SimplePredicate: (sameThreeCharStartPredicate, OWN_CITY)), (SimplePredicate: (fingerprint, OWN_ADDR), SimplePredicate: (oneGramFingerprint, OWN_STATE)))\n",
      "INFO:dedupe.blocking:10000, 0.1831782 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading learned settings from training/learned_settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.blocking:20000, 0.3771302 seconds\n",
      "INFO:dedupe.blocking:30000, 0.5649152 seconds\n",
      "INFO:dedupe.blocking:40000, 0.7501852 seconds\n",
      "INFO:dedupe.blocking:50000, 0.9332142 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sets 47615\n"
     ]
    }
   ],
   "source": [
    "settings_file = 'training/learned_settings'\n",
    "training_file = 'training/training.json'\n",
    "\n",
    "# If settings exist, read from existing.\n",
    "if os.path.exists(settings_file):\n",
    "    print('Reading learned settings from', settings_file)\n",
    "    with open(settings_file, 'rb') as f:\n",
    "        deduper = dedupe.StaticDedupe(f)\n",
    "else:\n",
    "    # Tell Dedupe which fields are used to identify duplicates.\n",
    "    fields = [\n",
    "        {'field': 'OWN_NAME_CL', 'type': 'String'},\n",
    "        {'field': 'OWN_ADDR', 'type': 'String'},\n",
    "        {'field': 'OWN_CITY', 'type': 'String'},\n",
    "        {'field': 'OWN_STATE', 'type': 'String'}\n",
    "        ]\n",
    "    deduper = dedupe.Dedupe(fields)\n",
    "    # If training file exists, read it...\n",
    "    if os.path.exists(training_file):\n",
    "        print('reading labeled examples from ', training_file)\n",
    "        with open(training_file, 'rb') as f:\n",
    "            deduper.prepare_training(df_dict, f)\n",
    "    # Otherwise, prepare a training set...\n",
    "    else:\n",
    "        deduper.prepare_training(df_dict)\n",
    "    # Start supervised labeling.\n",
    "    dedupe.console_label(deduper)\n",
    "    deduper.train()\n",
    "    # Write settings and training sets.\n",
    "    with open(training_file, 'w') as tf:\n",
    "        deduper.write_training(tf)\n",
    "    with open(settings_file, 'wb') as sf:\n",
    "        deduper.write_settings(sf)\n",
    "\n",
    "# Identify clusters based on training dataset.\n",
    "# Higher threshold is less tolerant of differences between names/addresses.\n",
    "clustered_dupes = deduper.partition(df_dict, threshold = 0.5)\n",
    "\n",
    "# How many sets are there?\n",
    "print('Number of sets', len(clustered_dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty arrays to hold results.\n",
    "rid = []\n",
    "clst = []\n",
    "conf = []\n",
    "count = []\n",
    "\n",
    "# Iterate over results...\n",
    "for cluster_id, (records, scores) in enumerate(clustered_dupes):\n",
    "    for record_id, score in zip(records, scores):\n",
    "        # How many properties does individual own?\n",
    "        count.append(len(records))\n",
    "        # Append record id\n",
    "        # Corresponds to index of assessor dataframe.\n",
    "        rid.append(record_id)\n",
    "        # Append cluster ID.\n",
    "        clst.append(cluster_id)\n",
    "        # Append confidence score.\n",
    "        conf.append(score)\n",
    "\n",
    "# Build new dataframe using result arrays.\n",
    "clust = pd.DataFrame(list(zip(clst, conf, count)), \n",
    "                  columns =['CLST', 'CONF', 'COUNT'],\n",
    "                  index = rid\n",
    "                 )\n",
    "\n",
    "# Join clusters to assessors dataframe.\n",
    "df = df.join(clust)\n",
    "\n",
    "# Write assessor's dataframe to CSV.\n",
    "df.to_csv('outputs/parcels_clustered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve clusters into owners list.\n",
    "owners = df.sort_values('CONF').groupby('CLST').agg(\n",
    "    # Owner with highest confidence.\n",
    "    own = ('OWNER1', 'first'),\n",
    "    # List of all unique owners.\n",
    "    own_list = ('OWNER1', 'unique'),\n",
    "    # Address with highest confidence\n",
    "    add = ('ADDR_FULL', 'first'),\n",
    "    # List of all unique addresses.\n",
    "    add_list = ('ADDR_FULL', 'unique')\n",
    ")\n",
    "\n",
    "# Write owners to CSV.\n",
    "owners.to_csv('outputs/owners.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read spatial data. Unified parcel data available from MassGIS\n",
    "# https://docs.digital.mass.gov/dataset/massgis-data-standardized-assessors-parcels\n",
    "parcels_gdf = gpd.read_file('data/mamas_parcels.shp')\n",
    "\n",
    "# Merge parcels and dataframe on 'LOC_ID' column\n",
    "# Right join because we want to duplicate parcels with multiple owners\n",
    "# E.g., condos, multiply-owned triple-deckers...\n",
    "parcels_joined = parcels_gdf.merge(df, on='LOC_ID', how='right')\n",
    "\n",
    "# Remove records without geometries.\n",
    "parcels_joined = parcels_joined[parcels_joined['geometry'] != None]\n",
    "\n",
    "# Dissolve polygons to multi-polygons on cluster.\n",
    "parcels_multi = parcels_joined[['CLST', 'geometry', 'COUNT']].dissolve(by='CLST')\n",
    "\n",
    "# Write parcel multi-polygons to file.\n",
    "parcels_multi.to_file(\"parcels_multi.shp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
