{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import dedupe\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PG_CONNECT = os.getenv(\"PG_CONNECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residential Land Use Codes from MA Dept of Revenue\n",
    "# https://www.mass.gov/files/documents/2016/08/wr/classificationcodebook.pdf\n",
    "# Codes are 101*-109*, 031*, and 013*\n",
    "# Often include suffixes (letters, zeroes or no character), thus regex *?\n",
    "USE_CODES = '^1[0-1][1-9]*?|^013*?|^031*?'\n",
    "BOS_CODES = '^R[1-4]$|^RC$|^RL$|^CD$|^A$'\n",
    "def read_res(file_dict):\n",
    "    df = pd.DataFrame()\n",
    "    for town, file, in file_dict.items():\n",
    "        town_df = gpd.read_file(file).drop('geometry', axis='columns')\n",
    "        town_df['town'] = town\n",
    "        df = df.append(town_df, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from MassGIS Standardized Assessor's Parcels\n",
    "# https://docs.digital.mass.gov/dataset/massgis-data-standardized-assessors-parcels\n",
    "# Medford, Cambridge, and Somerville all last updated FY 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somerville, Medford processing.\n",
    "files = {'som': './data/assess/som_assess.dbf', \n",
    "          'med': './data/assess/med_assess.dbf'}\n",
    "som_med = read_res(files)\n",
    "# Rename column to lower-case.\n",
    "som_med.columns = som_med.columns.str.lower()\n",
    "# Filter for residential parcels.\n",
    "som_med = som_med[som_med['use_code'].str.contains(USE_CODES, regex=True)]\n",
    "# Identify rows with co-owner names erroneously listed in address column.\n",
    "mask = som_med.own_addr.str.contains(pat = '|'.join(['^C/O', '^[A-Za-z]']), na=False) & ~som_med.own_addr.str.contains(pat = '|'.join(['^PO', '^P.O.', '^P. O.', '^P O ', '^ONE', '^BOX', '^ZERO']), na=False)\n",
    "# Add co-owners identified to co column.\n",
    "som_med['co'] = som_med.own_addr[mask]\n",
    "som_med.loc[~mask, 'co'] = None\n",
    "# Fill own_addr with none for above-identified rows.\n",
    "som_med.loc[mask, 'own_addr'] = None\n",
    "# Remame columns\n",
    "som_med = som_med.rename(columns = {\n",
    "    'prop_id': 'gisid',\n",
    "    'owner1': 'own',\n",
    "    'site_addr': 'prop_addr'\n",
    "})\n",
    "som_med.loc[:,'prop_addr'] = som_med.prop_addr.str.strip()\n",
    "# Replace underscores with hyphens.\n",
    "som_med.loc[:,'gisid'] = som_med.gisid.str.replace(r'_', '-', regex=True)\n",
    "# Concatenate address.\n",
    "som_med.loc[:,'own_addr'] = [', '.join((str(a), str(b), str(c))) for a, b, c in zip(som_med['own_addr'], som_med['own_city'], som_med['own_state'])]\n",
    "som_med.loc[:,'own_addr'] = [' '.join((str(a), str(b))) for a, b in zip(som_med['own_addr'], som_med['own_zip'])]\n",
    "# Remove concatenated Nones.\n",
    "som_med = som_med.replace({r'None, ': ''}, regex=True)\n",
    "# Filter columns.\n",
    "som_med = som_med[['gisid', 'town', 'prop_addr', 'own', 'co', 'own_addr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = pd.read_csv('./data/assess/bos_assess.csv', dtype={'GIS_ID': str, 'MAIL_ZIPCODE': str, 'U_TOT_RMS': str})\n",
    "bos.columns = bos.columns.str.lower()\n",
    "bos = bos.rename(columns = {\n",
    "    'gis_id': 'gisid',\n",
    "    'owner': 'own',\n",
    "    'mail_addressee': 'co'\n",
    "})\n",
    "bos['town'] = 'bos'\n",
    "# Filter by residential property types.\n",
    "bos = bos[bos['lu'].str.contains(BOS_CODES, regex=True)]\n",
    "bos.loc[:, 'gisid'] = bos.gisid.str.strip().str.pad(width=10, side='left', fillchar='0')\n",
    "# Pad ZIP code with zeroes, remove 4-digit suffix.\n",
    "bos.loc[:,'mail_zipcode'] = bos.mail_zipcode.astype(str).str.strip().str.pad(width=5, side='left', fillchar='0')\n",
    "# Add comma between city and state.\n",
    "bos.loc[:,'mail cs'] = bos['mail cs'].str.rsplit(' ', 1).apply(lambda x: ', '.join(x))\n",
    "# Concatenate property address components\n",
    "bos.loc[:,'prop_addr'] = [' '.join((str(a), str(b), str(c))) for a, b, c in zip(bos['st_num'], bos['st_name'], bos['st_name_suf'])]\n",
    "bos.loc[:,'prop_addr'] = [' #'.join((str(a), str(b))) for a, b in zip(bos['prop_addr'], bos['unit_num'])]\n",
    "bos.loc[:,'prop_addr'] = bos.prop_addr.str.strip()\n",
    "# Concatenate owner address components.\n",
    "bos.loc[:,'own_addr'] = [', '.join((str(a), str(b))) for a, b in zip(bos['mail_address'], bos['mail cs'])]\n",
    "bos.loc[:,'own_addr'] = [' '.join((str(a), str(b))) for a, b in zip(bos['own_addr'], bos['mail_zipcode'])]\n",
    "bos.loc[:,'own_addr'] = bos.own_addr.str.strip()\n",
    "# Filter columns\n",
    "bos = bos[['gisid', 'town', 'prop_addr', 'own', 'co', 'own_addr']]\n",
    "# Replace blank strings with None (necessary for dedupe).\n",
    "bos = bos.replace({' ': None, '': None, r' #nan': None})\n",
    "bos = bos.replace({r' #nan': ''}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = pd.read_csv('./data/assess/cam_assess.csv', dtype=str)\n",
    "# cam = cam.astype(str).replace(r'nan', '', regex=True)\n",
    "# rename all columns to lowercase\n",
    "cam.columns = cam.columns.str.lower()\n",
    "# Filter for residential properties.\n",
    "cam = cam[cam['stateclasscode'].str.contains(USE_CODES, regex=True)]\n",
    "# Pad zip to five digits and remove 4-digit zip suffix.\n",
    "cam.loc[:,'owner_zip'] = cam['owner_zip'].str.rsplit('-', 1).str[0]\n",
    "# Identify rows with co-owner names erroneously listed in address column.\n",
    "mask = cam.owner_address.str.contains(pat = '|'.join(['^C/O', '^ATTN:']), na=False)\n",
    "cam.loc[mask, 'owner_address'] = None\n",
    "# Add co-owners identified to co column.\n",
    "cam.loc[mask, 'owner_coownername'] = [', '.join((str(a), str(b)))  for a, b in zip(cam.loc[mask, 'owner_coownername'], cam.loc[mask, 'owner_address'])]\n",
    "# Concatenate owner address components\n",
    "cam.loc[:,'own_addr'] = [', '.join((str(a), str(b), str(c), str(d))) for a, b, c, d in zip(cam['owner_address'], cam['owner_address2'], cam['owner_city'], cam['owner_state'])]\n",
    "cam.loc[:,'own_addr'] = [' '.join((str(a), str(b))) for a, b in zip(cam['own_addr'], cam['owner_zip'])]\n",
    "cam.loc[:,'own_addr'] = cam.own_addr.str.strip()\n",
    "# Clean property address column\n",
    "cam['prop_addr'] = cam['address'].str.rsplit('(', 1).apply(lambda x: x[0].replace('\\n', ' ').strip())\n",
    "cam['town'] = 'cam' \n",
    "cam = cam.rename(columns = {\n",
    "    'owner_name': 'own',\n",
    "    'owner_coownername': 'co'\n",
    "})\n",
    "cam = cam.replace({r'^, ': '', r' ,': '', r', nan': '', r'None, ': '', r', None': ''}, regex=True)\n",
    "cam = cam.replace({' ': None, '': None, np.nan: None})\n",
    "cam = cam[['gisid', 'town', 'prop_addr', 'own', 'co', 'own_addr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brook = pd.read_csv('./data/assess/brook_assess.csv', dtype=str)\n",
    "brook.columns = brook.columns.str.lower()\n",
    "brook = brook[brook['usecd'].str.contains(USE_CODES, regex=True)]\n",
    "\n",
    "brook.loc[:,'zip'] = brook['zip'].str.rsplit('-', 1).str[0]\n",
    "# Name town.\n",
    "brook['town'] = 'brk' \n",
    "# Concatenate address.\n",
    "brook.loc[:,'own_addr'] = [', '.join((str(a), str(b), str(c))) for a, b, c, in zip(brook['address'], brook['city'], brook['state'])]\n",
    "# Append zip to address with no comma.\n",
    "brook.loc[:,'own_addr'] = [' '.join((str(a), str(b))) for a, b in zip(brook['own_addr'], brook['zip'])]\n",
    "brook.loc[:,'own_addr'] = brook.own_addr.str.strip()\n",
    "# Concatenate property address components\n",
    "brook.loc[:,'prop_addr'] = [''.join((str(a), str(b))) for a, b in zip(brook['addno1'], brook['addno2'])]\n",
    "brook.loc[:,'prop_addr'] = [' '.join((str(a), str(b))) for a, b in zip(brook['prop_addr'], brook['addst1'])]\n",
    "brook.loc[:,'prop_addr'] = [' '.join((str(a), str(b))) for a, b in zip(brook['prop_addr'], brook['addst2'])]\n",
    "brook.loc[:,'prop_addr'] = brook.prop_addr.str.strip()\n",
    "# Append \n",
    "brook.loc[:,'own'] = [' '.join((str(a), str(b))) for a, b in zip(brook['firstname1'], brook['lastname1'])]\n",
    "brook.loc[:,'co'] = [' '.join((str(a), str(b))) for a, b in zip(brook['firstname2'], brook['lastname2'])]\n",
    "brook = brook.replace({' ': None, '': None})\n",
    "brook = brook.rename(columns = {\n",
    "    'parcel-id': 'gisid'\n",
    "})\n",
    "brook = brook.replace({r'^, ': '', r' ,': '', r', nan': '', r'nan': '', r'None, ': '', r', None': ''}, regex=True)\n",
    "brook = brook.replace({' ': None, '': None})\n",
    "brook = brook[['gisid', 'town', 'prop_addr', 'own', 'co', 'own_addr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assess = pd.concat([som_med, cam, bos, brook], ignore_index=True)\n",
    "all_assess.loc[:,'prop_addr'] = all_assess.prop_addr.str.lstrip('0').str.strip()\n",
    "all_assess.loc[:,'own_addr'] = all_assess.own_addr.str.lstrip('0').str.strip()\n",
    "all_assess = all_assess.replace({r'None': '', 'nan': ''}, regex=True)\n",
    "all_assess = all_assess.replace({' ': None, '': None})\n",
    "all_assess = all_assess[~all_assess.own_addr.str.contains('|'.join(['^BOSTON, MA', '^MEDFORD, MA', '^SOMERVILLE, MA']), regex=True, na=False)]\n",
    "all_assess = all_assess[~pd.isnull(all_assess['gisid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dictionary (expected by Dedupe)\n",
    "all_assess_dict = all_assess.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((SimplePredicate: (oneGramFingerprint, own_addr), SimplePredicate: (sameThreeCharStartPredicate, own_addr)), (PartialPredicate: (commonTwoTokens, own, CorporationName), PartialPredicate: (twoGramFingerprint, own, CorporationName)), (PartialPredicate: (commonTwoTokens, co, CorporationName), SimplePredicate: (commonThreeTokens, own_addr)), (PartialPredicate: (commonFourGram, own, Surname), SimplePredicate: (twoGramFingerprint, own)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading learned settings from ./training/learned_settings\n"
     ]
    }
   ],
   "source": [
    "settings_file = './training/learned_settings'\n",
    "training_file = './training/training.json'\n",
    "\n",
    "# If settings exist, read from existing.\n",
    "if os.path.exists(settings_file):\n",
    "    print('Reading learned settings from', settings_file)\n",
    "    with open(settings_file, 'rb') as f:\n",
    "        deduper = dedupe.StaticDedupe(f)\n",
    "else:\n",
    "    # Tell Dedupe which fields are used to identify duplicates.\n",
    "    fields = [\n",
    "        {'field': 'own', 'variable name': 'own', 'type': 'Name'},\n",
    "        {'field': 'co', 'variable name': 'co', 'type': 'Name'},\n",
    "        {'field': 'own_addr', 'variable name': 'own_addr', 'type': 'Address'},\n",
    "        {'type': 'Interaction', 'interaction variables': ['own', 'co']}\n",
    "        ]\n",
    "    deduper = dedupe.Dedupe(fields)\n",
    "    # If training file exists, read it...\n",
    "    if os.path.exists(training_file):\n",
    "        print('reading labeled examples from ', training_file)\n",
    "        with open(training_file, 'rb') as f:\n",
    "            deduper.prepare_training(all_assess_dict, f)\n",
    "    # Otherwise, prepare a training set...\n",
    "    else:\n",
    "        deduper.prepare_training(all_assess_dict)\n",
    "    # Start supervised labeling.\n",
    "    dedupe.console_label(deduper)\n",
    "    deduper.train()\n",
    "    # Write settings and training sets.\n",
    "    with open(training_file, 'w') as tf:\n",
    "        deduper.write_training(tf)\n",
    "    with open(settings_file, 'wb') as sf:\n",
    "        deduper.write_settings(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.blocking:10000, 3.9768902 seconds\n",
      "INFO:dedupe.blocking:20000, 7.8477522 seconds\n",
      "INFO:dedupe.blocking:30000, 11.4433792 seconds\n",
      "INFO:dedupe.blocking:40000, 16.3532982 seconds\n",
      "INFO:dedupe.blocking:50000, 21.4108232 seconds\n",
      "INFO:dedupe.blocking:60000, 25.9323602 seconds\n",
      "INFO:dedupe.blocking:70000, 29.8725542 seconds\n",
      "INFO:dedupe.blocking:80000, 34.1600332 seconds\n",
      "INFO:dedupe.blocking:90000, 38.4827952 seconds\n",
      "INFO:dedupe.blocking:100000, 42.6273742 seconds\n",
      "INFO:dedupe.blocking:110000, 46.5384842 seconds\n",
      "INFO:dedupe.blocking:120000, 50.2511372 seconds\n",
      "INFO:dedupe.blocking:130000, 53.7834232 seconds\n",
      "INFO:dedupe.blocking:140000, 57.4280022 seconds\n",
      "INFO:dedupe.blocking:150000, 61.1980012 seconds\n",
      "INFO:dedupe.blocking:160000, 65.0863892 seconds\n",
      "INFO:dedupe.blocking:170000, 69.1169082 seconds\n",
      "INFO:dedupe.blocking:180000, 73.1619042 seconds\n",
      "INFO:dedupe.blocking:190000, 76.8786992 seconds\n",
      "INFO:dedupe.blocking:200000, 81.0180292 seconds\n",
      "INFO:dedupe.blocking:210000, 85.3040122 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sets 174932\n"
     ]
    }
   ],
   "source": [
    "# Identify clusters based on training dataset.\n",
    "# Higher threshold is less tolerant of differences between names/addresses.\n",
    "clustered_dupes = deduper.partition(all_assess_dict, threshold = 0.7)\n",
    "\n",
    "# How many sets are there?\n",
    "print('Number of sets', len(clustered_dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty arrays to hold results.\n",
    "rid = []\n",
    "clst = []\n",
    "conf = []\n",
    "count = []\n",
    "\n",
    "# Iterate over results...\n",
    "for cluster_id, (records, scores) in enumerate(clustered_dupes):\n",
    "    for record_id, score in zip(records, scores):\n",
    "        # Append record id\n",
    "        # Corresponds to index of assessor dataframe.\n",
    "        rid.append(record_id)\n",
    "        # Append cluster ID.\n",
    "        clst.append(cluster_id)\n",
    "        # Append confidence score.\n",
    "        conf.append(score)\n",
    "\n",
    "# Build new dataframe using result arrays.\n",
    "clust = pd.DataFrame(list(zip(clst, conf)), \n",
    "                  columns =['clst', 'conf'],\n",
    "                  index = rid\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join clusters to assessors dataframe.\n",
    "all_assess = all_assess.join(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-adcc4c67cce9>:8: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  parcels_gdf.loc[:,'geometry'] = parcels_gdf.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "# Read spatial data\n",
    "parcels_gdf = gpd.read_file('./data/parcels/mamas_parcels.shp')\n",
    "parcels_gdf = parcels_gdf.rename(columns = {\n",
    "    'pid': 'gisid'\n",
    "}).drop_duplicates(subset=['gisid', 'town'])\n",
    "parcels_gdf = parcels_gdf[~pd.isnull(parcels_gdf['gisid'])]\n",
    "parcels_gdf = parcels_gdf[~pd.isnull(parcels_gdf['geometry'])]\n",
    "parcels_gdf.loc[:,'geometry'] = parcels_gdf.geometry.centroid\n",
    "parcels_gdf.loc[:,'lat'] = parcels_gdf.geometry.y\n",
    "parcels_gdf.loc[:,'lon'] = parcels_gdf.geometry.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assess = parcels_gdf.merge(all_assess, on=['town', 'gisid'], how='right')\n",
    "all_assess = all_assess[~np.isnan(all_assess.lat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_engine = create_engine(PG_CONNECT)\n",
    "all_assess.to_postgis(\"props\", con=pg_engine, schema='public', if_exists='fail', index=True, index_label='id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
