{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dedupe\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residential Land Use Codes from MA Dept of Revenue\n",
    "# https://www.mass.gov/files/documents/2016/08/wr/classificationcodebook.pdf\n",
    "# Codes are 101*-109*, 031*, and 013*\n",
    "# Often include suffixes (letters, zeroes or no character), thus regex *?\n",
    "USE_CODES = '^1[0-1][1-9]*?|^013*?|^031*?'\n",
    "def read_res(file_list, uses = USE_CODES):\n",
    "    df = pd.DataFrame()\n",
    "    for file in file_list:\n",
    "        df = df.append(gpd.read_file(file), ignore_index=True)\n",
    "    df = df[df['USE_CODE'].str.contains(uses, regex=True)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from MassGIS Standardized Assessor's Parcels\n",
    "# https://docs.digital.mass.gov/dataset/massgis-data-standardized-assessors-parcels\n",
    "# Medford, Cambridge, and Somerville all last updated FY 2019\n",
    "files = ['data/som_assess.dbf', 'data/cam_assess.dbf', 'data/med_assess.dbf']\n",
    "df = read_res(files)\n",
    "df_dict = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_list = ['FAMILY', 'IRREVOCABLE', 'NOMINEE', 'REVOCABLE', \n",
    "                'REALTY', 'REAL ESTATE', 'TRUSTEES OF', 'TRUSTEE OF', \n",
    "                'TRUSTEE', 'TRST', 'TRUST', 'LTD', 'LLC', 'HOLDINGS', 'REALTORS', 'LIMITED PARTNERSHIP', \n",
    "                'FOR LIFE', 'LIFE ESTATE', 'ESTATE OF', 'ESTATE', 'TR.']\n",
    "def clean(c):\n",
    "    c = c.replace('|'.join(map(re.escape, replace_list)), '', regex=True)\n",
    "    return c\n",
    "\n",
    "df['OWN_NAME_CL'] = clean(df['OWNER1'])\n",
    "df.head()\n",
    "df_dict = df.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_file = 'training/learned_settings'\n",
    "training_file = 'training/training.json'\n",
    "\n",
    "if os.path.exists(settings_file):\n",
    "    print('Reading learned settings from', settings_file)\n",
    "    with open(settings_file, 'rb') as f:\n",
    "        deduper = dedupe.StaticDedupe(f)\n",
    "else:\n",
    "    fields = [\n",
    "        {'field': 'OWN_NAME_CL', 'type': 'String'},\n",
    "        {'field': 'OWN_ADDR', 'type': 'String'},\n",
    "        {'field': 'OWN_CITY', 'type': 'String'},\n",
    "        {'field': 'OWN_STATE', 'type': 'String'}\n",
    "        ]\n",
    "    deduper = dedupe.Dedupe(fields)\n",
    "    if os.path.exists(training_file):\n",
    "        print('reading labeled examples from ', training_file)\n",
    "        with open(training_file, 'rb') as f:\n",
    "            deduper.prepare_training(df_dict, f)\n",
    "    else:\n",
    "        deduper.prepare_training(df_dict)\n",
    "    print('Starting active labeling...')\n",
    "    dedupe.console_label(deduper)\n",
    "    deduper.train()\n",
    "    with open(training_file, 'w') as tf:\n",
    "        deduper.write_training(tf)\n",
    "    with open(settings_file, 'wb') as sf:\n",
    "        deduper.write_settings(sf)\n",
    "\n",
    "print('Clustering...')\n",
    "clustered_dupes = deduper.partition(df_dict, 0.5)\n",
    "print('Number of sets', len(clustered_dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = []\n",
    "clst = []\n",
    "conf = []\n",
    "count = []\n",
    "for cluster_id, (records, scores) in enumerate(clustered_dupes):\n",
    "    for record_id, score in zip(records, scores):\n",
    "        count.append(len(records))\n",
    "        rid.append(record_id)\n",
    "        clst.append(cluster_id)\n",
    "        conf.append(score)\n",
    "        \n",
    "clust = pd.DataFrame(list(zip(clst, conf, count)), \n",
    "                  columns =['CLST', 'CONF', 'COUNT'],\n",
    "                  index = rid\n",
    "                 )\n",
    "df = df.join(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('LOC_ID')\n",
    "df.to_csv('outputs/parcels_clustered.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
