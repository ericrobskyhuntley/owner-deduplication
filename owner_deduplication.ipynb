{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import dedupe\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PG_CONNECT = os.getenv(\"PG_CONNECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residential Land Use Codes from MA Dept of Revenue\n",
    "# https://www.mass.gov/files/documents/2016/08/wr/classificationcodebook.pdf\n",
    "# Codes are 101*-109*, 031*, and 013*\n",
    "# Often include suffixes (letters, zeroes or no character), thus regex *?\n",
    "USE_CODES = '^1[0-1][1-9]*?|^013*?|^031*?'\n",
    "BOS_CODES = '^R[1-4]$|^RC$|^RL$|^CD$|^A$'\n",
    "\n",
    "medparse = lambda x: pd.datetime.strptime(x, '%Y%m%d')\n",
    "\n",
    "def read_res(file_dict):\n",
    "    df = pd.DataFrame()\n",
    "    for town, file, in file_dict.items():\n",
    "        town_df = gpd.read_file(file).drop('geometry', axis='columns')\n",
    "        town_df['town'] = town\n",
    "        df = df.append(town_df, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from MassGIS Standardized Assessor's Parcels\n",
    "# https://docs.digital.mass.gov/dataset/massgis-data-standardized-assessors-parcels\n",
    "# Medford, Cambridge, and Somerville all last updated FY 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medford processing.\n",
    "files = {'med': './data/assess/med_assess.dbf'}\n",
    "med = read_res(files)\n",
    "# Rename column to lower-case.\n",
    "med.columns = med.columns.str.lower()\n",
    "# Filter for residential paWorldrcels.\n",
    "med = med[med['use_code'].str.contains(USE_CODES, regex=True)]\n",
    "# Identify rows with co-owner names erroneously listed in address column.\n",
    "mask = med.own_addr.str.contains(pat = '|'.join(['^C/O', '^[A-Za-z]']), na=False) & ~med.own_addr.str.contains(pat = '|'.join(['^PO', '^P.O.', '^P. O.', '^P O ', '^ONE', '^BOX', '^ZERO']), na=False)\n",
    "# Add co-owners identified to co column.\n",
    "med['co'] = med.own_addr[mask]\n",
    "med.loc[~mask, 'co'] = None\n",
    "# Fill own_addr with none for above-identified rows.\n",
    "med.loc[mask, 'own_addr'] = None\n",
    "# Remame columns\n",
    "med = med.rename(columns = {\n",
    "    'prop_id': 'gisid',\n",
    "    'owner1': 'own',\n",
    "    'site_addr': 'prop_addr',\n",
    "    'total_val': 'ass_val',\n",
    "    'location': 'unit',\n",
    "    'ls_date': 'sale_d',\n",
    "    'ls_price': 'sale_p'\n",
    "})\n",
    "med.loc[:,'sale_d'] = pd.to_datetime(med['sale_d'], format='%Y%m%d')\n",
    "med.loc[:,'prop_addr'] = med.prop_addr.str.strip()\n",
    "# Replace underscores with hyphens.\n",
    "med.loc[:,'gisid'] = med.gisid.str.replace(r'_', '-', regex=True)\n",
    "# Concatenate address.\n",
    "med.loc[:,'own_addr'] = [', '.join((str(a), str(b), str(c))) for a, b, c in zip(med['own_addr'], med['own_city'], med['own_state'])]\n",
    "med.loc[:,'own_addr'] = [' '.join((str(a), str(b))) for a, b in zip(med['own_addr'], med['own_zip'])]\n",
    "# Remove concatenated Nones.\n",
    "med = med.replace({r'None, ': ''}, regex=True)\n",
    "med['year'] = 'FY2019'\n",
    "med.loc[:,'sale_p'] = med['sale_p'].replace(0, None)\n",
    "# Filter columns.\n",
    "med = med[['gisid', 'town', 'prop_addr', 'unit', 'own', 'co', 'own_addr', 'ass_val', 'year', 'sale_d', 'sale_p']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somerville processing.\n",
    "som = pd.read_csv('./data/assess/som_assess_FY14-FY19.csv',\n",
    "                  dtype={'HOUSE NO': str}) \n",
    "som.columns = som.columns.str.lower()\n",
    "\n",
    "# Filter for residential parcels.\n",
    "som = som[som['pcc'].str.contains(USE_CODES, regex=True)]\n",
    "\n",
    "som.loc[:,'prop_addr'] = [' '.join((str(a), str(b))) for a, b in zip(som['house no'], som['street'])]\n",
    "\n",
    "som = som.replace({r'^, ': '', r' ,': '', r', nan': '', r'nan': '', r'None, ': '', r', None': ''}, regex=True)\n",
    "som = som.replace({' ': None, '': None, np.nan: None})\n",
    "\n",
    "# Pad ZIP code with zeroes, remove 4-digit suffix.\n",
    "# Assessor appears to have overzealously corrected...\n",
    "som.loc[:,'owner zip'] = som['owner zip'].str[1:]\n",
    "som.loc[:,'own_addr'] = [', '.join((str(a), str(b), str(c), str(d))) for a, b, c, d in zip(som['owner add'], som['owner city'], som['owner state'], som['owner zip'])]\n",
    "\n",
    "som.loc[:,'gisid'] = ['-'.join((str(m), str(b), str(l))) for m, b, l in zip(som['map'], som['block'], som['lot'])]\n",
    "som['town'] = 'som'\n",
    "som = som.drop(['year'], axis=1)\n",
    "som = som.rename(columns = {\n",
    "    'commitment owner': 'own',\n",
    "    'current co-owner': 'co',\n",
    "    'parcel val': 'ass_val',\n",
    "    'fiscal_year': 'year'\n",
    "})\n",
    "\n",
    "# Assessor seems to have screwed up this column in the 2014-2019 data\n",
    "# but it appears that 2019 data is incrementally numbered (¯\\_(ツ)_/¯)\n",
    "som = som.loc[som['year'] >= 2019]\n",
    "som['year'] = 'FY2019'\n",
    "# Filter columns.\n",
    "som = som[['gisid', 'town', 'prop_addr', 'unit', 'own', 'co', 'own_addr', 'ass_val', 'year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somerville 2019 assessor's table doesn't include sale date \n",
    "# (apparently by accident), so we collate with MassGIS source.\n",
    "files = {'som': './data/assess/som_massgis.dbf'}\n",
    "som_mg = read_res(files)\n",
    "# Rename column to lower-case.\n",
    "som_mg.columns = som_mg.columns.str.lower()\n",
    "# Remame columns\n",
    "som_mg = som_mg.rename(columns = {\n",
    "    'prop_id': 'gisid',\n",
    "    'ls_date': 'sale_d',\n",
    "    'ls_price': 'sale_p'\n",
    "})\n",
    "som_mg.loc[:,'sale_d'] = pd.to_datetime(som_mg['sale_d'], format='%Y%m%d')\n",
    "# Replace underscores with hyphens.\n",
    "som_mg.loc[:,'gisid'] = som_mg.gisid.str.replace(r'_', '-', regex=True)\n",
    "som_mg.loc[:,'sale_p'] = som_mg['sale_p'].replace(0, None)\n",
    "# Filter columns.\n",
    "som_mg = som_mg[['gisid', 'sale_d', 'sale_p']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = som.merge(som_mg[['gisid', 'sale_d', 'sale_p']], how='left', on='gisid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = pd.read_csv('./data/assess/bos_assess.csv', dtype={'GIS_ID': str, 'MAIL_ZIPCODE': str, 'U_TOT_RMS': str})\n",
    "bos.columns = bos.columns.str.lower()\n",
    "bos = bos.rename(columns = {\n",
    "    'gis_id': 'gisid',\n",
    "    'owner': 'own',\n",
    "    'mail_addressee': 'co',\n",
    "    'unit_num': 'unit',\n",
    "    'av_total': 'ass_val'\n",
    "})\n",
    "bos['town'] = 'bos'\n",
    "# Filter by residential property types.\n",
    "bos = bos[bos['lu'].str.contains(BOS_CODES, regex=True)]\n",
    "bos.loc[:, 'gisid'] = bos.gisid.str.strip().str.pad(width=10, side='left', fillchar='0')\n",
    "# Pad ZIP code with zeroes, remove 4-digit suffix.\n",
    "bos.loc[:,'mail_zipcode'] = bos.mail_zipcode.astype(str).str.strip().str.pad(width=5, side='left', fillchar='0')\n",
    "# Add comma between city and state.\n",
    "bos.loc[:,'mail cs'] = bos['mail cs'].str.rsplit(' ', 1).apply(lambda x: ', '.join(x))\n",
    "# Concatenate property address components\n",
    "bos.loc[:,'prop_addr'] = [' '.join((str(a), str(b), str(c))) for a, b, c in zip(bos['st_num'], bos['st_name'], bos['st_name_suf'])]\n",
    "bos.loc[:,'prop_addr'] = bos.prop_addr.str.strip()\n",
    "# Concatenate owner address components.\n",
    "bos.loc[:,'own_addr'] = [', '.join((str(a), str(b))) for a, b in zip(bos['mail_address'], bos['mail cs'])]\n",
    "bos.loc[:,'own_addr'] = [' '.join((str(a), str(b))) for a, b in zip(bos['own_addr'], bos['mail_zipcode'])]\n",
    "bos.loc[:,'own_addr'] = bos.own_addr.str.strip()\n",
    "# Filter columns\n",
    "bos['year'] = 'FY2020'\n",
    "bos = bos[['gisid', 'town', 'prop_addr', 'unit', 'own', 'co', 'own_addr', 'ass_val', 'year']]\n",
    "# Replace blank strings with None (necessary for dedupe).\n",
    "bos = bos.replace({' ': None, '': None, r' #nan': None})\n",
    "bos = bos.replace({r' #nan': ''}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = pd.read_csv('./data/assess/cam_assess.csv',\n",
    "                  parse_dates=['SaleDate'],\n",
    "                  dtype={'Owner_Zip': str, \n",
    "                         'SalePrice': float,\n",
    "                         'StateClassCode': str\n",
    "                        })\n",
    "# rename all columns to lowercase\n",
    "cam.columns = cam.columns.str.lower()\n",
    "# Filter for residential properties.\n",
    "cam = cam[cam['stateclasscode'].str.contains(USE_CODES, regex=True)]\n",
    "# Pad zip to five digits and remove 4-digit zip suffix.\n",
    "cam.loc[:,'owner_zip'] = cam['owner_zip'].str.rsplit('-', 1).str[0]\n",
    "# Identify rows with co-owner names erroneously listed in address column.\n",
    "mask = cam.owner_address.str.contains(pat = '|'.join(['^C/O', '^ATTN:']), na=False)\n",
    "cam.loc[mask, 'owner_address'] = None\n",
    "# Add co-owners identified to co column.\n",
    "cam.loc[mask, 'owner_coownername'] = [', '.join((str(a), str(b)))  for a, b in zip(cam.loc[mask, 'owner_coownername'], cam.loc[mask, 'owner_address'])]\n",
    "# Concatenate owner address components\n",
    "cam.loc[:,'own_addr'] = [', '.join((str(a), str(b), str(c), str(d))) for a, b, c, d in zip(cam['owner_address'], cam['owner_address2'], cam['owner_city'], cam['owner_state'])]\n",
    "cam.loc[:,'own_addr'] = [' '.join((str(a), str(b))) for a, b in zip(cam['own_addr'], cam['owner_zip'])]\n",
    "cam.loc[:,'own_addr'] = cam.own_addr.str.strip()\n",
    "# Clean property address column\n",
    "cam['prop_addr'] = cam['address'].str.rsplit('\\nCambridge, MA', 1).apply(lambda x: x[0].replace('\\n', ' ').strip())\n",
    "# Bring property address in line with others.\n",
    "cam['town'] = 'cam' \n",
    "cam = cam.rename(columns = {\n",
    "    'owner_name': 'own',\n",
    "    'owner_coownername': 'co',\n",
    "    'assessedvalue': 'ass_val',\n",
    "    'saleprice': 'sale_p',\n",
    "    'saledate': 'sale_d'\n",
    "})\n",
    "cam['year'] = 'FY2020'\n",
    "cam['sale_p'].values[cam['sale_p'].values < 1] = None\n",
    "cam = cam.replace({r'^, ': '', r' ,': '', r', nan': '', r'None, ': '', r', None': ''}, regex=True)\n",
    "cam = cam.replace({' ': None, '': None, np.nan: None})\n",
    "cam = cam[['gisid', 'town', 'prop_addr', 'unit', 'own', 'co', 'own_addr', 'ass_val', 'year', 'sale_d', 'sale_p']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brook = pd.read_csv('./data/assess/brook_assess.csv', \n",
    "                    dtype={'SALEPRICE': float,\n",
    "                          'USECD': str},\n",
    "                    parse_dates=['SALEDATE'])\n",
    "brook.columns = brook.columns.str.lower()\n",
    "brook = brook[brook['usecd'].str.contains(USE_CODES, regex=True)]\n",
    "\n",
    "brook.loc[:,'zip'] = brook['zip'].str.rsplit('-', 1).str[0]\n",
    "# Name town.\n",
    "brook['town'] = 'brk' \n",
    "# Concatenate address.\n",
    "brook.loc[:,'own_addr'] = [', '.join((str(a), str(b), str(c))) for a, b, c, in zip(brook['address'], brook['city'], brook['state'])]\n",
    "# Append zip to address with no comma.\n",
    "brook.loc[:,'own_addr'] = [' '.join((str(a), str(b))) for a, b in zip(brook['own_addr'], brook['zip'])]\n",
    "brook.loc[:,'own_addr'] = brook.own_addr.str.strip()\n",
    "# Concatenate property address components\n",
    "brook.loc[:,'prop_addr'] = [''.join((str(a), str(b))) for a, b in zip(brook['addno1'], brook['addno2'])]\n",
    "brook.loc[:,'prop_addr'] = [' '.join((str(a), str(b))) for a, b in zip(brook['prop_addr'], brook['addst1'])]\n",
    "brook.loc[:,'prop_addr'] = brook.prop_addr.str.strip()\n",
    "# Append \n",
    "brook.loc[:,'own'] = [' '.join((str (a), str(b))) for a, b in zip(brook['firstname1'], brook['lastname1'])]\n",
    "brook.loc[:,'co'] = [' '.join((str(a), str(b))) for a, b in zip(brook['firstname2'], brook['lastname2'])]\n",
    "brook = brook.replace({' ': None, '': None})\n",
    "brook = brook.rename(columns = {\n",
    "    'parcel-id': 'gisid',\n",
    "    'addst2': 'unit',\n",
    "    'restotlval': 'ass_val',\n",
    "    'saleprice': 'sale_p',\n",
    "    'saledate': 'sale_d'\n",
    "})\n",
    "brook = brook.replace({r'^, ': '', r' ,': '', r', nan': '', r'nan': '', r'None, ': '', r', None': ''}, regex=True)\n",
    "brook = brook.replace({' ': None, '': None})\n",
    "brook['sale_p'].values[brook['sale_p'].values < 1] = None\n",
    "brook['year'] = 'FY2020'\n",
    "brook = brook[['gisid', 'town', 'prop_addr', 'unit', 'own', 'co', 'own_addr', 'ass_val', 'year', 'sale_d', 'sale_p']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assess = pd.concat([som, med, cam, bos, brook], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assess.loc[:,'prop_addr'] = all_assess.prop_addr.str.lstrip('0').str.strip()\n",
    "all_assess.loc[:,'own_addr'] = all_assess.own_addr.str.lstrip('0').str.strip()\n",
    "all_assess.loc[:,'co'] = all_assess['co'].replace({r'C/O ': '', r'S/O ': '', r'ATTN: ': '', r'ATTN ': ''}, regex=True)\n",
    "all_assess = all_assess.replace({r'None': '', 'nan': ''}, regex=True)\n",
    "all_assess = all_assess.replace({' ': None, '': None})\n",
    "all_assess = all_assess[~pd.isnull(all_assess['gisid'])]\n",
    "all_assess = all_assess.replace({pd.np.nan: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tupleize(row):\n",
    "    if (row['co'] is not None) & (row['own'] is not None):\n",
    "        return tuple([row['own'], row['co']])\n",
    "    elif (row['co'] is None) & (row['own'] is not None):\n",
    "        return tuple([row['own']])\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "all_assess['owners'] = all_assess.apply(tupleize, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dictionary (expected by Dedupe)\n",
    "all_assess_dict = all_assess[['owners','own_addr']].to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_file = './training/learned_settings'\n",
    "training_file = './training/training.json'\n",
    "\n",
    "# If settings exist, read from existing.\n",
    "if os.path.exists(settings_file):\n",
    "    print('Reading learned settings from', settings_file)\n",
    "    with open(settings_file, 'rb') as f:\n",
    "        deduper = dedupe.StaticDedupe(f)\n",
    "else:\n",
    "    # Tell Dedupe which fields are used to identify duplicates.\n",
    "    fields = [\n",
    "        {'field': 'owners', 'variable name': 'owners', 'type': 'Set'},\n",
    "        {'field': 'own_addr', 'variable name': 'own_addr', 'type': 'Address'}\n",
    "        ]\n",
    "    deduper = dedupe.Dedupe(fields)\n",
    "    # If training file exists, read it...\n",
    "    if os.path.exists(training_file):\n",
    "        print('reading labeled examples from ', training_file)\n",
    "        with open(training_file, 'rb') as f:\n",
    "            deduper.prepare_training(all_assess_dict, f)\n",
    "    # Otherwise, prepare a training set...\n",
    "    else:\n",
    "        deduper.prepare_training(all_assess_dict)\n",
    "    # Start supervised labeling.\n",
    "    dedupe.console_label(deduper)\n",
    "    deduper.train()\n",
    "    # Write settings and training sets.\n",
    "    with open(training_file, 'w') as tf:\n",
    "        deduper.write_training(tf)\n",
    "    with open(settings_file, 'wb') as sf:\n",
    "        deduper.write_settings(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify clusters based on training dataset.\n",
    "# Higher threshold is less tolerant of differences between names/addresses.\n",
    "clustered_dupes = deduper.partition(all_assess_dict, threshold = 0.5)\n",
    "\n",
    "# How many sets are there?\n",
    "print('Number of sets', len(clustered_dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty arrays to hold results.\n",
    "rid = []\n",
    "clst = []\n",
    "conf = []\n",
    "count = []\n",
    "\n",
    "# Iterate over results...\n",
    "for cluster_id, (records, scores) in enumerate(clustered_dupes):\n",
    "    for record_id, score in zip(records, scores):\n",
    "        # Append record id\n",
    "        # Corresponds to index of assessor dataframe.\n",
    "        rid.append(record_id)\n",
    "        # Append cluster ID.\n",
    "        clst.append(cluster_id)\n",
    "        # Append confidence score.\n",
    "        conf.append(score)\n",
    "\n",
    "# Build new dataframe using result arrays.\n",
    "clust = pd.DataFrame(list(zip(clst, conf)), \n",
    "                  columns =['clst', 'conf'],\n",
    "                  index = rid\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join clusters to assessors dataframe.\n",
    "all_assess = all_assess.join(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read spatial data\n",
    "parcels_gdf = gpd.read_file('./data/parcels/mamas_parcels.shp')\n",
    "parcels_gdf = parcels_gdf.rename(columns = {\n",
    "    'pid': 'gisid'\n",
    "}).drop_duplicates(subset=['gisid', 'town'])\n",
    "parcels_gdf = parcels_gdf[~pd.isnull(parcels_gdf['gisid'])]\n",
    "parcels_gdf = parcels_gdf[~pd.isnull(parcels_gdf['geometry'])]\n",
    "# parcels_gdf.loc[:,'geometry'] = parcels_gdf.geometry.centroid\n",
    "centroid = parcels_gdf.geometry.centroid\n",
    "parcels_gdf.loc[:,'lat'] = centroid.y\n",
    "parcels_gdf.loc[:,'lon'] = centroid.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assess = parcels_gdf.merge(all_assess, on=['town', 'gisid'], how='right')\n",
    "all_assess = all_assess[~np.isnan(all_assess.lat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coding this count logic saves a ton of time for each PostgreSQL query.\n",
    "all_assess = all_assess.merge(all_assess.groupby('clst').count()[['gisid']].rename(columns={'gisid': 'count'}), on=['clst', 'clst'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_engine = create_engine(PG_CONNECT)\n",
    "all_assess.to_postgis(\"props\", con=pg_engine, schema='public', if_exists='replace', index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
