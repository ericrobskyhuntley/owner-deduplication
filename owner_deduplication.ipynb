{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residential Land Use Codes from MA Dept of Revenue\n",
    "# https://www.mass.gov/files/documents/2016/08/wr/classificationcodebook.pdf\n",
    "# Codes are 101*-109*, 031*, and 013*\n",
    "# Often include suffixes (letters, zeroes or no character), thus regex *?\n",
    "USE_CODES = '^1[0-1][1-9]*?|^013*?|^031*?'\n",
    "def read_res(file_list, uses = USE_CODES):\n",
    "    df = pd.DataFrame()\n",
    "    for file in file_list:\n",
    "        df = df.append(gpd.read_file(file), ignore_index=True)\n",
    "    df = df[[\n",
    "        'LOC_ID',\n",
    "        'OWNER1',\n",
    "        'OWN_ADDR',\n",
    "        'OWN_CITY',\n",
    "        'OWN_STATE',\n",
    "        'OWN_ZIP',\n",
    "        'OWN_CO',\n",
    "        'USE_CODE',\n",
    "        'CITY',\n",
    "        'FY']]\n",
    "    df = df.rename({\n",
    "        'LOC_ID': 'loc_id',\n",
    "        'OWNER1': 'own_name', \n",
    "        'OWN_ADDR': 'own_add', \n",
    "        'OWN_CITY': 'own_city', \n",
    "        'OWN_STATE': 'own_state',\n",
    "        'OWN_ZIP': 'own_zip', \n",
    "        'OWN_CO': 'own_country',\n",
    "        'CITY': 'city',\n",
    "        'USE_CODE': 'use',\n",
    "        'FY': 'year'}, \n",
    "        axis='columns')\n",
    "    df = df[df['use'].str.contains(uses, regex=True)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from MassGIS Standardized Assessor's Parcels\n",
    "# https://docs.digital.mass.gov/dataset/massgis-data-standardized-assessors-parcels\n",
    "# Medford, Cambridge, and Somerville all last updated FY 2019\n",
    "files = ['data/som_assess.dbf', 'data/cam_assess.dbf', 'data/med_assess.dbf']\n",
    "df = read_res(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "replace_list = ['FAMILY', 'IRREVOCABLE', 'NOMINEE', 'REVOCABLE', \n",
    "                'REALTY', 'REAL ESTATE', 'TRUSTEES OF', 'TRUSTEE OF', \n",
    "                'TRUSTEE', 'TRST', 'TRUST', 'LTD', 'LLC', 'HOLDINGS', 'REALTORS', 'LIMITED PARTNERSHIP', \n",
    "                'FOR LIFE', 'ESTATE OF', 'ESTATE', 'TR.']\n",
    "def clean(c):\n",
    "    c = c.replace('|'.join(map(re.escape, replace_list)), '', regex=True)\n",
    "    return c\n",
    "\n",
    "df['own_name_clean'] = clean(df['own_name'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dedupe\n",
    "\n",
    "fields = [\n",
    "    {'field': 'own_name_clean', 'type': 'String'},\n",
    "    {'field': 'own_add', 'type': 'String'},\n",
    "    {'field': 'own_city', 'type': 'String'},\n",
    "    {'field': 'own_state', 'type': 'String'}\n",
    "    ]\n",
    "\n",
    "df_dict = df.to_dict('index')\n",
    "\n",
    "deduper = dedupe.Dedupe(fields)\n",
    "\n",
    "deduper.prepare_training(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedupe.console_label(deduper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.train()\n",
    "with open('training/training.json', 'w') as tf:\n",
    "    deduper.write_training(tf)\n",
    "with open('training/settings', 'wb') as sf:\n",
    "    deduper.write_settings(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Clustering...')\n",
    "clustered_dupes = deduper.partition(df_dict, 0.5)\n",
    "print('# duplicate sets', len(clustered_dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = []\n",
    "clst = []\n",
    "conf = []\n",
    "count = []\n",
    "for cluster_id, (records, scores) in enumerate(clustered_dupes):\n",
    "    for record_id, score in zip(records, scores):\n",
    "        count.append(len(records))\n",
    "        rid.append(record_id)\n",
    "        clst.append(cluster_id)\n",
    "        conf.append(score)\n",
    "        \n",
    "clust = pd.DataFrame(list(zip(clst, conf, count)), \n",
    "                  columns =['clst', 'conf', 'count'],\n",
    "                  index = rid\n",
    "                 )\n",
    "df = df.join(clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('loc_id')\n",
    "df.to_csv('outputs/parcels_clustered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
